---
title: "Tornadoes threathen health, floods destroy property."
author: "Sander Schrieken"
date: "10/30/2017"
output: html_document
---

## Summary

This report analyses the effects of severe weather events in the US. The [NOAA](http://www.noaa.gov) Storm Database was used to determine which weather events cause injury, death, property and crop damage. The database covers the period 1950 - 2011. It provides information on more than 900,000 weather events. After we removed improper entries from the dataset we were left with about 635,000 events. These events were grouped by event type, after which the total number of injuries / fatalities per type were calculated. The total damage to property / crops was produced too. As was the number of events in each group. The analysis shows that [tornadoes](https://en.wikipedia.org/wiki/Tornado) have the biggest impact on public health, since they are responsible for the highest number of injuries and fatalities. [Floods](https://en.wikipedia.org/wiki/Flood) cause most of the damage to property and crops.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Data Processing

The code below downloads the data from the Coursera webpage for the Reproducible Research Course Project (#2). This is the [link](https://www.coursera.org/learn/reproducible-research/peer/OMZ37/course-project-2) to that page.

```{r download_(meta)_data, message=FALSE, warning=FALSE, eval=FALSE, cache=TRUE}
Download_Date <- Sys.Date() # First, fix the date we download the data!

source_url <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
download.file(source_url, destfile = "StormData.csv.bz2", 
              method = "curl")

source_url_2 <- "https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf"
download.file(source_url_2, destfile = "repdata-peer2_doc-pd01016005curr.pdf", 
              method = "curl")

source_url_3 <- "https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf"
download.file(source_url_3,
              destfile = "repdata-peer2_doc-NCDC Storm Events-FAQ Page.pdf",
              method = "curl")
```

Since the names of the files containing meta-data are unwieldy we will rename them:

```{r rename_meta_data_files, echo=TRUE, results="hide", message=FALSE, warning=FALSE, cache=TRUE}
# The names of the files are unwieldy; we should consider renaming them...
file.rename("repdata-peer2_doc-pd01016005curr.pdf", "Storm Data Documentation.pdf")
file.rename("repdata-peer2_doc-NCDC Storm Events-FAQ Page.pdf", "Storm Events FAQs.pdf")
```

Reading the downloaded data into R can be done with `read.csv()`. By using this function we don't have to unzip the compressed file separately, since `read.csv()` can handle this for us. The following code reads the downloaded data into R. We will _NOT_ run it, however, because this is slow.

```{r importing_StormData_read.csv, cache=TRUE, eval=FALSE}
StormData <- read.csv("StormData.csv.bz2", stringsAsFactors = FALSE)
```

The `data.table` package provides a much faster way of getting the data into R, so we
will use that instead:

```{r install_data.table, message=FALSE, warning=FALSE}
options(repos = "https://cran.rstudio.com")
if (!require(data.table)) install.packages("data.table")
library(data.table)
```

Note that `fread()` can not handle the unzipping for us. We will have to use the `bunzip2` shell command for that. Luckily `fread()` can handle shell commands and process the output:

```{r importing_StormData_fread, message=FALSE, warning=FALSE, cache=TRUE}
StormData <- fread('bunzip2 -ck StormData.csv.bz2')
```

[next: analysing the dataset, pointing out issues (dmg exponents, number of event types)]

StormData contains `r length(names(StormData))` variables and `r nrow(StormData)` observations. Using `dplyr`, we will reduce the number of variables by removing the irrelevant colums:

```{r install_dplyr, message=FALSE, warning=FALSE, cache=TRUE}
options(repos = "https://cran.rstudio.com")
if (!require(dplyr)) install.packages("dplyr")
library(dplyr)
```

```{r select_relevant_variables, message=FALSE, warning=FALSE, cache=TRUE}
StormData <- StormData %>%
  select(EVTYPE, FATALITIES, INJURIES, PROPDMG, PROPDMGEXP, CROPDMG, CROPDMGEXP)
```

So we've reduced the number of variables / columns to `r length(names(StormData))`. Let's sample some of it to see what's in there:

```{r sample_StormData, message=FALSE, warning=FALSE, cache=TRUE}
StormData[sample(nrow(StormData), 8),]
```

Let's look at the variable `EVTYPE`. On page 6 of the Storm Data Documentation we find the Storm Data Event Table with all the possible event types. In total there are 48 possible entries. However, if we calculate the number of unique event types present in our dataset we get `r length(unique(StormData$EVTYPE))` of them. That is way more than we would expect. So what's going on here? let's sample some of the data:

```{r sample_unique_EVTYPEs, message=FALSE, warning=FALSE, cache=TRUE}
unique(StormData$EVTYPE)[sample(length(unique(StormData$EVTYPE)), 9)]
```

There appears to be some [cruft](https://www.urbandictionary.com/define.php?term=cruft) â€” data values that have not been selected from the Storm Data Event Table. The majority of the `EVTYPE` values entered have been typed in UPPER CASE, so this should be considered the prescribed way of entering data. Most entries conform, but a number of them are (partly) in lower case.

Since it's not possible to look at all of the extra unique values for `EVTYPE` separately and transform each one to a valid entry if possible we will get rid of the non-conforming values. We will try to match the `EVTYPE` values in the dataset with the list of valid values (as contained in the Storm Data Event Table). Values that match, but contain lower case letters will be considered valid entries. Exact matches too, of course.

We've created a character vector with all the valid `EVTYPE` values by copying the list from page 6 of the Storm Data Documentation file. We used a texteditor to whip things into shape. In oder to keep everything reproducible, here's what we did:

- Found " Z ", " C ", and " M " in copied text. Replaced that with newline characters to get a list.
- Removed " Z" from the last values (copied two lists seperately).
- Corrected apparent typo in "V olcanic Ash" (removed space between V and olcanic).
- Wrapped each line in quotes.
- Added a comma plus one space to the end of each line except the last one.
- Removed all newline characters from the list; copied the single line of text to R.

Subsequently we assigned this list to the variable `Allowed_EVTYPEs`:

```{r create_variable_Allowed_EVTYPEs, message=FALSE, warning=FALSE, cache=TRUE}
Allowed_EVTYPEs <- c("Astronomical Low Tide",
                     "Avalanche",
                     "Blizzard", "Coastal Flood", "Cold/Wind Chill",
                     "Debris Flow", "Dense Fog", "Dense Smoke",
                     "Drought", "Dust Devil", "Dust Storm",
                     "Excessive Heat", "Extreme Cold/Wind Chill",
                     "Flash Flood", "Flood",
                     "Frost/Freeze", "Funnel Cloud", "Freezing Fog",
                     "Hail", "Heat", "Heavy Rain", "Heavy Snow",
                     "High Surf", "High Wind", "Hurricane (Typhoon)",
                     "Ice Storm", "Lake-Effect Snow", "Lakeshore Flood",
                     "Lightning", "Marine Hail", "Marine High Wind",
                     "Marine Strong Wind", "Marine Thunderstorm Wind",
                     "Rip Current", "Seiche", "Sleet", "Storm Surge/Tide",
                     "Strong Wind", "Thunderstorm Wind", "Tornado",
                     "Tropical Depression", "Tropical Storm", "Tsunami",
                     "Volcanic Ash", "Waterspout", "Wildfire", "Winter Storm",
                     "Winter Weather")
```

So there are indeed `r length(Allowed_EVTYPEs)` possible values for `EVTYPE`, or double that amount if we accept both lower and upper versions. To this end we convert the vector to a data.frame and add a column containing the same values in upper case:

```{r expand_Allowed_EVTYPEs_with_UPPER_CASE, message=FALSE, warning=FALSE, cache=TRUE}
Allowed_EVTYPEs <- data.frame(LowerCase = Allowed_EVTYPEs, UpperCase = toupper(Allowed_EVTYPEs))
```

Let's take a peek (sample) at what this data.frame contains:

```{r sample_Allowed_EVTYPEs, message=FALSE, warning=FALSE, cache=TRUE}
Allowed_EVTYPEs[sample(nrow(Allowed_EVTYPEs), 5),]
```

Now that we have the list of allowed event types we can filter the original dataset:

```{r filter_Allowed_EVTYPEs, message=FALSE, warning=FALSE, cache=TRUE}
StormData <- StormData %>%
  filter(EVTYPE %in% Allowed_EVTYPEs$LowerCase | EVTYPE %in% Allowed_EVTYPEs$UpperCase)
```

This has substantially reduced the size of the dataset:

```{r structure_cleaned_StormData_1, message=FALSE, warning=FALSE, cache=TRUE}
str(StormData)
```

We now have `r nrow(StormData)` observations left.

There are some event types set in lower case. Let's fix that, so that we can group the event types properly:

```{r set_all_EVTYPEs_to_UPPER, message=FALSE, warning=FALSE, cache=TRUE}
StormData$EVTYPE <- toupper(StormData$EVTYPE)
```

We are confronted with a similar issue when we look at the values that have been entered for the variables `PROPDMGEXP` and `CROPDMGEXP`. The allowed values are "K", "M", and "B", but there are more than 3 unique entries. `r length(unique(StormData$PROPDMGEXP))` and `r length(unique(StormData$CROPDMGEXP))` respectively. Non conforming entries can't be used to calculate the actual amount of damage, so we have to ignore those values. Observations without any damage have "" as a value for the damage exponents, so we'll set variable to the same for non conforming entries. When proper values ("K/k", "M/m", or "B/b") are entered we'll transform these values to 1000, 1,000,000 and 1,000,000,000 respectively so that we can calculate and compare the property / crop damage per event type.

```{r transform_PROPDMGEXP, message=FALSE, warning=FALSE, cache=TRUE}
StormData$PROPDMGEXP<- ifelse(StormData$PROPDMGEXP %in% c("K", "k"), 1000,
                    ifelse(StormData$PROPDMGEXP %in% c("M", "m"), 1000000,
                  ifelse(StormData$PROPDMGEXP %in% c("B", "b"), 1000000000, "")))
```

```{r transform_CROPDMGEXP, message=FALSE, warning=FALSE, cache=TRUE}
StormData$CROPDMGEXP<- ifelse(StormData$CROPDMGEXP %in% c("K", "k"), 1000,
                    ifelse(StormData$CROPDMGEXP %in% c("M", "m"), 1000000,
                  ifelse(StormData$CROPDMGEXP %in% c("B", "b"), 1000000000, "")))
```

No that we have numbers as exponent values we can calculate the damage to property and crops per event. After calculating `PROPDMG` and `CROPDMG` by multiplying the values by the exponents we can do away with the `PROPDMGEXP` and `CROPDMGEXP` variables.

```{r calculating_PROPDMG_CROPDMG, message=FALSE, warning=FALSE, cache=TRUE}
StormData$PROPDMG <- StormData$PROPDMG * as.integer(StormData$PROPDMGEXP)
StormData$CROPDMG <- StormData$CROPDMG * as.integer(StormData$CROPDMGEXP)
```

Now group the data by `EVTYPE` and calculate:

- number of events per group
- total fatalities and injuries for each group
- average fatalities and injuries per group (per event)
- total property and crop damage for each group
- average property and crop damage per group (per event)

```{r group_by_EVTYPEs, message=FALSE, warning=FALSE, cache=TRUE}
StormData_SUMMARY <- StormData %>%
  group_by(EVTYPE) %>%
  summarise(COUNT = n(), PCT = round(COUNT/nrow(StormData)*100, 1),
            TOTAL_FAT = sum(FATALITIES), TOTAL_INJ = sum(INJURIES),
            PROPDMG_PCT = round(sum(PROPDMG, na.rm = TRUE) /
                  sum(StormData$PROPDMG, na.rm = TRUE) * 100, 0),
            CROPDMG_PCT = round(sum(CROPDMG, na.rm = TRUE) /
                  sum(StormData$CROPDMG, na.rm = TRUE) * 100, 0))
StormData_SUMMARY[, 1:5]
StormData_SUMMARY[, c(1:3, 6:7)]
```

Please note that the colums `PROPDMG_PCT` and `CROPDMG_PCT` display the relative share of the total property / crop damage. Absolute figures are extremely large (milions of bilions) and go beyond comprehension. The percentage of the total damage does convey meaning, however.

## Results

We can now arrange the `StormData_SUMMARY` so that becomes clear which even types cause the most fatalities, injuries, property and crop damage. But first let's order the list by number / percentage of occurences.

```{r SUMMARY_by_COUNT, message=FALSE, warning=FALSE, cache=TRUE}
StormData_COUNT <- arrange(StormData_SUMMARY, desc(COUNT))
```

```{r SUMMARY_by_FATALITIES, message=FALSE, warning=FALSE, cache=TRUE}
StormData_FATALITIES <- arrange(StormData_SUMMARY, desc(TOT_FAT))
```

StormData_Stacked_Barplot <- StormData_Grouped_By_EVTYPE[, c("EVTYPE", "TOT_PROPDMG", "TOT_CROPDMG")]

StormData_Stacked_Barplot <- melt(StormData_Stacked_Barplot, id = "EVTYPE")

ggplot(StormData_Stacked_Barplot, aes(x = EVTYPE, y = value, fill = variable)) + geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5,size=8,face="bold"),
  panel.grid.major.x = element_blank()) +
scale_y_continuous(labels=comma)

StormData_Stacked_Barplot_2 <- StormData_Grouped_By_EVTYPE[, c("EVTYPE", "TOT_FAT", "TOT_INJ")]

StormData_Stacked_Barplot_2 <- melt(StormData_Stacked_Barplot_2, id = "EVTYPE")

ggplot(StormData_Stacked_Barplot_2, aes(x = EVTYPE, y = value, fill = variable)) + geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5,size=8,face="bold"),
  panel.grid.major.x = element_blank()) +
scale_y_continuous(labels=comma)

Found the solution / recipe here: https://stackoverflow.com/questions/30949896/creating-stacked-barplots-in-r-using-different-variables

Figure Captions?

{r cars, fig.cap = "This caption is hard coded. Note no number is added."}

## Results

```{r install_ggplot2, message=FALSE, warning=FALSE}
options(repos = "https://cran.rstudio.com")
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)
```

Preliminary thoughts on the matter:

Which types of events are most harmful to population health???

There seem to be two variables that affect population health: FATALITIES and INJURIES. Possibly events with many fatalities will also show a lot of injuries, but we don't know. Something to find oud. There could be a difference in the top 10 events causing fatalities and injuries. Is an event with many injuries more harmful to population health than an event without injuries where one person died? That's a difficult question to answer, so it's probably best analyse fatalities and injuries together. It would be nice if there was a strong correlation. When there are obvious differences, we should point them out.

Drawing a plot wit injuries (x) and fatalities (y) shows a picture that apparently lacks significant correlation. The other way around (fatalies on X, and injuries on Y) doesn't change that. So there's hardly any meaningful correlation between number of injuries and number of fatalities. The 1995 heatwave resulted in almost 600 deaths, but very few injuries by comparison. On the other hand the 1979 Tornado (Whichita, TX) caused 1700 injuries and 'only' 42 fatalities.

Looking at the totals TORNADO events top the list in both fatalities and injuries. When looking at average number of injuries / fatalities per type of event TORNADO's score higher aswel (more fatalities). Heat-related events cause a higer number of injuries on average, but they occur way less often. Because TORNADO's are very common they have the most impact on the health of the population. See [DO].R and EDA.R for analyses and calculations.



## Review Criteria

1. Has either a (1) valid RPubs URL pointing to a data analysis document for this assignment been submitted; or (2) a complete PDF file presenting the data analysis been uploaded?
2. Is the document written in English?
3. Does the analysis include description and justification for any data transformations?
4. Does the document have a title that briefly summarizes the data analysis?
5. Does the document have a synopsis that describes and summarizes the data analysis in less than 10 sentences?
6. Is there a section titled "Data Processing" that describes how the data were loaded into R and processed for analysis?
7. Is there a section titled "Results" where the main results are presented?
8. Is there at least one figure in the document that contains a plot?
9. Are there at most 3 figures in this document?
10. Does the analysis start from the raw data file (i.e. the original .csv.bz2 file)?
11. Does the analysis address the question of which types of events are most harmful to population health?
12. Does the analysis address the question of which types of events have the greatest economic consequences?
13. Do all the results of the analysis (i.e. figures, tables, numerical summaries) appear to be reproducible?
14. Do the figure(s) have descriptive captions (i.e. there is a description near the figure of what is happening in the figure)?
15. As far as you can determine, does it appear that the work submitted for this project is the work of the student who submitted it?
